{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5> <b> SMU Homewwork 3 - MSDS7337 - Natural Language Processing </b> </font>\n",
    "<br><br><br>\n",
    "<center><font size=4> <b> Spring 2021  Ikenna Nwaogu </b></font> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://cdn.searchenginejournal.com/wp-content/uploads/2020/08/an-introduction-to-natural-language-processing-with-python-for-seos-5f3519eeb8368-760x400.webp\"> \n",
    "<br>\n",
    "<p align=\"center\"><font size=4> <b><center> Edit Distance And Stemming  </b></font> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Edit Distance and Percent Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit distance is away to compare 2 strings or words to see how similar or dissimalr they are. Percent match is a percentage representation of how closely the 2 values are. Below is the formular used for both edit distance and percentage match. Levenshtein edit-distance was used to calculate the edit distance. My first name is Ikenna and I was called Ikemore for a long time so I figured I go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance:  4\n",
      "Percentage String Match:  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import operator\n",
    "\n",
    "name = 'Ikenna'\n",
    "nickname = 'Ikemore'\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i  # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j  # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _edit_dist_step(lev, i, j, s1, s2, substitution_cost=1, transpositions=False):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (substitution_cost if c1 != c2 else 0)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and i > 1 and j > 1:\n",
    "        if s1[i - 2] == c2 and s2[j - 2] == c1:\n",
    "            d = lev[i - 2][j - 2] + 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "    \n",
    "def edit_distance(string1, string2, substitution_cost=1, transpositions=False):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    Allows specifying the cost of substitution edits (e.g., \"a\" -> \"b\"),\n",
    "    because sometimes it makes sense to assign greater penalties to\n",
    "    substitutions.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type substitution_cost: int\n",
    "    :type transpositions: bool\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(string1)\n",
    "    len2 = len(string2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(\n",
    "                lev,\n",
    "                i + 1,\n",
    "                j + 1,\n",
    "                string1,\n",
    "                string2,\n",
    "                substitution_cost=substitution_cost,\n",
    "                transpositions=transpositions,\n",
    "            )\n",
    "    return lev[len1][len2]\n",
    "\n",
    "\n",
    "editDistance = edit_distance(name, nickname)\n",
    "print(\"Edit Distance: \", editDistance)\n",
    "\n",
    "percent = (len(nickname) - editDistance) / (len(nickname))\n",
    "print(\"Percentage String Match: \", percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rewrite Sentences\n",
    "The books to be used are Neil Tyson Degrasse's Astrophysics for people in a hurry and Barack Obama's Dreams from My Father. \n",
    "\n",
    "The first two sentences for Neil's book are \"In the beginning, nearly fourteen billion years ago, all the space and all the matter and all the energy of the known universe was contained in a volume less than one-trillionth the size of the period that ends this sentence. Conditions were so hot, the basic forces of nature that collectively describes the universe were unified.\"\n",
    "\n",
    "The first two sentences for Barack's book are \"A few months after my twenty-first birthday a stranger called to give me the news. I was living in New York at the time, on Ninety-fourth between Second and First, part of that unnamed, shifting border between East Harlem and the rest of Manhattan.\"\n",
    "\n",
    "Rewording of Neil's 2 sentences:\n",
    "All nature's forces in the beginining of the universe shows that the universe were unified and all it's energy and matter were contained in a volume very much much smaller than a dot marked by a pencil.\n",
    "\n",
    "Rewording of Barack's 2 sentences:\n",
    "When I was living in New York between East Harlem and Manhattan, someone called me a few months afterI celebrated my twenty-first birthday to tell me the news.\n",
    "\n",
    "My friend definitely was able to pick Neil's book. She guessed Neil's book correctly and couldn't tell Barack Obama's book.\n",
    "\n",
    "My friend was able to correctly guess Neil's book because the words associated with sentences are astrophysics related. Neil is also known to use a very discriptive words.\n",
    "She couldn't guess Obama's book because of the generalization of the words used in the book. There are many authors that lived in New York. One stand out might be the word \"news\". The word might have given an exposure to what the book is about but I belive it isn't enough to give a better insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "\n",
    "We will look at 2 different stemmers provided by NLTK, the Porter and the Snowball Stemmer. Snowball stemmer has more options. It can ignore stopwords. The \"english\" stemmer option is better than the original \"porter\" stemmer option according to the NLTK website. I couldn't see the difference between having the ignore the stops words options because it included all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Potter-Neil-------------------------\n",
      "In  :  In\n",
      "the  :  the\n",
      "beginning  :  begin\n",
      ",  :  ,\n",
      "nearly  :  nearli\n",
      "fourteen  :  fourteen\n",
      "billion  :  billion\n",
      "years  :  year\n",
      "ago  :  ago\n",
      ",  :  ,\n",
      "all  :  all\n",
      "the  :  the\n",
      "space  :  space\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "matter  :  matter\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "energy  :  energi\n",
      "of  :  of\n",
      "the  :  the\n",
      "known  :  known\n",
      "universe  :  univers\n",
      "was  :  wa\n",
      "contained  :  contain\n",
      "in  :  in\n",
      "a  :  a\n",
      "volume  :  volum\n",
      "less  :  less\n",
      "than  :  than\n",
      "one-trillionth  :  one-trillionth\n",
      "the  :  the\n",
      "size  :  size\n",
      "of  :  of\n",
      "the  :  the\n",
      "period  :  period\n",
      "that  :  that\n",
      "ends  :  end\n",
      "this  :  thi\n",
      "sentence  :  sentenc\n",
      ".  :  .\n",
      "Conditions  :  condit\n",
      "were  :  were\n",
      "so  :  so\n",
      "hot  :  hot\n",
      ",  :  ,\n",
      "the  :  the\n",
      "basic  :  basic\n",
      "forces  :  forc\n",
      "of  :  of\n",
      "nature  :  natur\n",
      "that  :  that\n",
      "collectively  :  collect\n",
      "describes  :  describ\n",
      "the  :  the\n",
      "universe  :  univers\n",
      "were  :  were\n",
      "unified  :  unifi\n",
      ".  :  .\n",
      "----------------Porter-Barrack-------------------------\n",
      "A  :  A\n",
      "few  :  few\n",
      "months  :  month\n",
      "after  :  after\n",
      "my  :  my\n",
      "twenty-first  :  twenty-first\n",
      "birthday  :  birthday\n",
      "a  :  a\n",
      "stranger  :  stranger\n",
      "called  :  call\n",
      "to  :  to\n",
      "give  :  give\n",
      "me  :  me\n",
      "the  :  the\n",
      "news  :  news\n",
      ".  :  .\n",
      "I  :  I\n",
      "was  :  wa\n",
      "living  :  live\n",
      "in  :  in\n",
      "New  :  new\n",
      "York  :  york\n",
      "at  :  at\n",
      "the  :  the\n",
      "time  :  time\n",
      ",  :  ,\n",
      "on  :  on\n",
      "Ninety-fourth  :  ninety-fourth\n",
      "between  :  between\n",
      "Second  :  second\n",
      "and  :  and\n",
      "First  :  first\n",
      ",  :  ,\n",
      "part  :  part\n",
      "of  :  of\n",
      "that  :  that\n",
      "unnamed  :  unnam\n",
      ",  :  ,\n",
      "shifting  :  shift\n",
      "border  :  border\n",
      "between  :  between\n",
      "East  :  east\n",
      "Harlem  :  harlem\n",
      "and  :  and\n",
      "the  :  the\n",
      "rest  :  rest\n",
      "of  :  of\n",
      "Manhattan  :  manhattan\n",
      ".  :  .\n",
      "----------------SnowBall-Neil-------------------------\n",
      "In  :  in\n",
      "the  :  the\n",
      "beginning  :  begin\n",
      ",  :  ,\n",
      "nearly  :  near\n",
      "fourteen  :  fourteen\n",
      "billion  :  billion\n",
      "years  :  year\n",
      "ago  :  ago\n",
      ",  :  ,\n",
      "all  :  all\n",
      "the  :  the\n",
      "space  :  space\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "matter  :  matter\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "energy  :  energi\n",
      "of  :  of\n",
      "the  :  the\n",
      "known  :  known\n",
      "universe  :  univers\n",
      "was  :  was\n",
      "contained  :  contain\n",
      "in  :  in\n",
      "a  :  a\n",
      "volume  :  volum\n",
      "less  :  less\n",
      "than  :  than\n",
      "one-trillionth  :  one-trillionth\n",
      "the  :  the\n",
      "size  :  size\n",
      "of  :  of\n",
      "the  :  the\n",
      "period  :  period\n",
      "that  :  that\n",
      "ends  :  end\n",
      "this  :  this\n",
      "sentence  :  sentenc\n",
      ".  :  .\n",
      "Conditions  :  condit\n",
      "were  :  were\n",
      "so  :  so\n",
      "hot  :  hot\n",
      ",  :  ,\n",
      "the  :  the\n",
      "basic  :  basic\n",
      "forces  :  forc\n",
      "of  :  of\n",
      "nature  :  natur\n",
      "that  :  that\n",
      "collectively  :  collect\n",
      "describes  :  describ\n",
      "the  :  the\n",
      "universe  :  univers\n",
      "were  :  were\n",
      "unified  :  unifi\n",
      ".  :  .\n",
      "----------------SnowBall-Barrack-------------------------\n",
      "A  :  a\n",
      "few  :  few\n",
      "months  :  month\n",
      "after  :  after\n",
      "my  :  my\n",
      "twenty-first  :  twenty-first\n",
      "birthday  :  birthday\n",
      "a  :  a\n",
      "stranger  :  stranger\n",
      "called  :  call\n",
      "to  :  to\n",
      "give  :  give\n",
      "me  :  me\n",
      "the  :  the\n",
      "news  :  news\n",
      ".  :  .\n",
      "I  :  i\n",
      "was  :  was\n",
      "living  :  live\n",
      "in  :  in\n",
      "New  :  new\n",
      "York  :  york\n",
      "at  :  at\n",
      "the  :  the\n",
      "time  :  time\n",
      ",  :  ,\n",
      "on  :  on\n",
      "Ninety-fourth  :  ninety-fourth\n",
      "between  :  between\n",
      "Second  :  second\n",
      "and  :  and\n",
      "First  :  first\n",
      ",  :  ,\n",
      "part  :  part\n",
      "of  :  of\n",
      "that  :  that\n",
      "unnamed  :  unnam\n",
      ",  :  ,\n",
      "shifting  :  shift\n",
      "border  :  border\n",
      "between  :  between\n",
      "East  :  east\n",
      "Harlem  :  harlem\n",
      "and  :  and\n",
      "the  :  the\n",
      "rest  :  rest\n",
      "of  :  of\n",
      "Manhattan  :  manhattan\n",
      ".  :  .\n",
      "----------------Ignore-stop-words-SnowBall-Neil-------------------------\n",
      "In  :  in\n",
      "the  :  the\n",
      "beginning  :  begin\n",
      ",  :  ,\n",
      "nearly  :  near\n",
      "fourteen  :  fourteen\n",
      "billion  :  billion\n",
      "years  :  year\n",
      "ago  :  ago\n",
      ",  :  ,\n",
      "all  :  all\n",
      "the  :  the\n",
      "space  :  space\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "matter  :  matter\n",
      "and  :  and\n",
      "all  :  all\n",
      "the  :  the\n",
      "energy  :  energi\n",
      "of  :  of\n",
      "the  :  the\n",
      "known  :  known\n",
      "universe  :  univers\n",
      "was  :  was\n",
      "contained  :  contain\n",
      "in  :  in\n",
      "a  :  a\n",
      "volume  :  volum\n",
      "less  :  less\n",
      "than  :  than\n",
      "one-trillionth  :  one-trillionth\n",
      "the  :  the\n",
      "size  :  size\n",
      "of  :  of\n",
      "the  :  the\n",
      "period  :  period\n",
      "that  :  that\n",
      "ends  :  end\n",
      "this  :  this\n",
      "sentence  :  sentenc\n",
      ".  :  .\n",
      "Conditions  :  condit\n",
      "were  :  were\n",
      "so  :  so\n",
      "hot  :  hot\n",
      ",  :  ,\n",
      "the  :  the\n",
      "basic  :  basic\n",
      "forces  :  forc\n",
      "of  :  of\n",
      "nature  :  natur\n",
      "that  :  that\n",
      "collectively  :  collect\n",
      "describes  :  describ\n",
      "the  :  the\n",
      "universe  :  univers\n",
      "were  :  were\n",
      "unified  :  unifi\n",
      ".  :  .\n",
      "----------------Ignore-stop-words-SnowBall-Barrack-------------------------\n",
      "A  :  a\n",
      "few  :  few\n",
      "months  :  month\n",
      "after  :  after\n",
      "my  :  my\n",
      "twenty-first  :  twenty-first\n",
      "birthday  :  birthday\n",
      "a  :  a\n",
      "stranger  :  stranger\n",
      "called  :  call\n",
      "to  :  to\n",
      "give  :  give\n",
      "me  :  me\n",
      "the  :  the\n",
      "news  :  news\n",
      ".  :  .\n",
      "I  :  i\n",
      "was  :  was\n",
      "living  :  live\n",
      "in  :  in\n",
      "New  :  new\n",
      "York  :  york\n",
      "at  :  at\n",
      "the  :  the\n",
      "time  :  time\n",
      ",  :  ,\n",
      "on  :  on\n",
      "Ninety-fourth  :  ninety-fourth\n",
      "between  :  between\n",
      "Second  :  second\n",
      "and  :  and\n",
      "First  :  first\n",
      ",  :  ,\n",
      "part  :  part\n",
      "of  :  of\n",
      "that  :  that\n",
      "unnamed  :  unnam\n",
      ",  :  ,\n",
      "shifting  :  shift\n",
      "border  :  border\n",
      "between  :  between\n",
      "East  :  east\n",
      "Harlem  :  harlem\n",
      "and  :  and\n",
      "the  :  the\n",
      "rest  :  rest\n",
      "of  :  of\n",
      "Manhattan  :  manhattan\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords  \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "ss = SnowballStemmer(\"english\")\n",
    "sss = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "NeilSentence = \"In the beginning, nearly fourteen billion years ago, all the space and all the matter and all the energy of the known universe was contained in a volume less than one-trillionth the size of the period that ends this sentence. Conditions were so hot, the basic forces of nature that collectively describes the universe were unified.\"\n",
    "BarackSentence = \"A few months after my twenty-first birthday a stranger called to give me the news. I was living in New York at the time, on Ninety-fourth between Second and First, part of that unnamed, shifting border between East Harlem and the rest of Manhattan.\"\n",
    "\n",
    "NeilWords = word_tokenize(NeilSentence)\n",
    "BarackWords = word_tokenize(BarackSentence)\n",
    "\n",
    "print('----------------Potter-Neil-------------------------')\n",
    "for w in NeilWords: \n",
    "    print(w, \" : \", ps.stem(w)) \n",
    "\n",
    "print('----------------Porter-Barrack-------------------------')\n",
    "\n",
    "for w in BarackWords: \n",
    "    print(w, \" : \", ps.stem(w)) \n",
    "\n",
    "print('----------------SnowBall-Neil-------------------------')\n",
    "\n",
    "for w in NeilWords: \n",
    "    print(w, \" : \", ss.stem(w)) \n",
    "\n",
    "print('----------------SnowBall-Barrack-------------------------')\n",
    "\n",
    "for w in BarackWords: \n",
    "    print(w, \" : \", ss.stem(w)) \n",
    " \n",
    "print('----------------Ignore-stop-words-SnowBall-Neil-------------------------')\n",
    "\n",
    "for w in NeilWords: \n",
    "    print(w, \" : \", sss.stem(w)) \n",
    "\n",
    "print('----------------Ignore-stop-words-SnowBall-Barrack-------------------------')\n",
    "for w in BarackWords: \n",
    "    print(w, \" : \", sss.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referrence\n",
    "1. www.nltk.org/_modules/nltk/metrics/distance.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
