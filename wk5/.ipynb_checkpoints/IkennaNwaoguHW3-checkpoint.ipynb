{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5> <b> SMU Homewwork 2 - MSDS7337 - Natural Language Processing </b> </font>\n",
    "<br><br><br>\n",
    "<center><font size=4> <b> Spring 2021  Ikenna Nwaogu </b></font> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://cdn.searchenginejournal.com/wp-content/uploads/2020/08/an-introduction-to-natural-language-processing-with-python-for-seos-5f3519eeb8368-760x400.webp\"> \n",
    "<br>\n",
    "<p align=\"center\"><font size=4> <b><center> Lexical Diversity, Vocabulary Size and New Text Difficulty Score  </b></font> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocabulary size Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaliztion can be done using Min Max Scaler from the sklearn module. It can also be done using the simple normalization formula. The formula is dividing the difference of each text's vocabulary size to the min vocabular size on the list by the difference between the max and the min vocabulary size. Texts from nltk package were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Method to get the Vocab Size and normalize the score\n",
    "\n",
    "def n_vocab_size(*arg):\n",
    "    v_size = np.array([])\n",
    "    v_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        v_size = np.append(v_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using formula \n",
    "    for vsize in v_size:\n",
    "        v_size_norm = np.append(v_size_norm,(vsize - v_size.min()) /\n",
    "                                                    (v_size.max() - v_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn module\n",
    "    v_size_norm_sklearn = minmax_scale(v_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(v_size,v_size_norm,v_size_norm_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = n_vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size\n",
      "- 19317.0\n",
      "- 6833.0\n",
      "- 2789.0\n",
      "- 9913.0\n",
      "- 6066.0\n",
      "- 2166.0\n",
      "- 12408.0\n",
      "- 1108.0\n",
      "- 6807.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size\", *vocab_size[0],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normaliztion using the simple formula\n",
      "- 1.0\n",
      "- 0.31440496457795597\n",
      "- 0.09231698610577187\n",
      "- 0.4835520896260091\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944808\n",
      "- 0.0\n",
      "- 0.31297709923664124\n"
     ]
    }
   ],
   "source": [
    "print(\"Normaliztion using the simple formula\", *vocab_size[1],sep='\\n- ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization using the sklearn module\n",
      "- 0.9999999999999999\n",
      "- 0.3144049645779559\n",
      "- 0.09231698610577188\n",
      "- 0.48355208962600904\n",
      "- 0.2722829370091713\n",
      "- 0.05810313581196112\n",
      "- 0.6205722444944807\n",
      "- 0.0\n",
      "- 0.3129770992366412\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalization using the sklearn module\", *vocab_size[2],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Long Word Vocabulary Size and Normalization With Default Length 10.\n",
    "\n",
    "Below we used the same normalization methods as we used with the overall vocabulary size. Using the texts from nltk package we computed the long word vocabulary size. The function written below computes the values and normalization values of the long word vocabulary sizes of multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_long_vocab_size(*arg, longWordLength = 10):\n",
    "    v_size = np.array([])\n",
    "    v_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        V = set(text)\n",
    "        long_words = [w for w in V if len(w) > longWordLength]\n",
    "        v_size = np.append(v_size,len(set(long_words)))\n",
    "    \n",
    "    #### Normalizing using formula \n",
    "    for vsize in v_size:\n",
    "        v_size_norm = np.append(v_size_norm,(vsize - v_size.min()) /\n",
    "                                                    (v_size.max() - v_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn module\n",
    "    v_size_norm_sklearn = minmax_scale(v_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(v_size,v_size_norm,v_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_vocab_size = n_long_vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9,longWordLength = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size\n",
      "- 1912.0\n",
      "- 724.0\n",
      "- 64.0\n",
      "- 1378.0\n",
      "- 259.0\n",
      "- 45.0\n",
      "- 1463.0\n",
      "- 38.0\n",
      "- 478.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size\", *long_vocab_size[0],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normaliztion using the simple formula\n",
      "- 1.0\n",
      "- 0.3660618996798292\n",
      "- 0.013874066168623266\n",
      "- 0.7150480256136607\n",
      "- 0.11792956243329776\n",
      "- 0.003735325506937033\n",
      "- 0.7604055496264674\n",
      "- 0.0\n",
      "- 0.23479188900747064\n"
     ]
    }
   ],
   "source": [
    "print(\"Normaliztion using the simple formula\", *long_vocab_size[1],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization using the sklearn module\n",
      "- 1.0000000000000002\n",
      "- 0.3660618996798293\n",
      "- 0.013874066168623266\n",
      "- 0.7150480256136607\n",
      "- 0.11792956243329776\n",
      "- 0.0037353255069370317\n",
      "- 0.7604055496264674\n",
      "- 0.0\n",
      "- 0.23479188900747067\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalization using the sklearn module\", *long_vocab_size[2],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Difficulty Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section a new scoring metrics were used. It is made up by combining the lexical score with the normalized score of the vocabulary size and the long-word vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(*args):\n",
    "    lex_div = np.array([])\n",
    "    for text in args:\n",
    "        val = len(set(text)) / len(text)\n",
    "        lex_div = np.append(val)\n",
    "    return(lex_div)\n",
    "\n",
    "def text_difficulty_score(*args, longWordLength = 10):\n",
    "    lex_div = np.array([])\n",
    "    longword_voc = np.array([])\n",
    "    voc_size = np.array([])\n",
    "    v_size_norm = np.array([])\n",
    "    df_scores_norm = np.array([])\n",
    "    longword_voc_norm = np.array([])\n",
    "    \n",
    "    for text in args:\n",
    "        V = set(text)\n",
    "        val = len(set(text)) / len(text)\n",
    "        lex_div = np.append(lex_div,val)\n",
    "        long_words = [w for w in V if len(w) > longWordLength]\n",
    "        longword_voc = np.append(longword_voc,len(set(long_words)))\n",
    "        voc_size = np.append(voc_size,len(set(text)))\n",
    "        \n",
    "    # Normalize the score of vocabulary size\n",
    "    for vsize in voc_size:\n",
    "        v_size_norm = np.append(v_size_norm,(vsize - voc_size.min()) /\n",
    "                                                    (voc_size.max() - voc_size.min()))\n",
    "    \n",
    "    # Normalize the long word score of vocabulary size\n",
    "    for vsize in longword_voc:\n",
    "        longword_voc_norm = np.append(longword_voc_norm,(vsize - longword_voc.min()) /\n",
    "                                                    (longword_voc.max() - longword_voc.min()))\n",
    "    df_scores = lex_div + longword_voc + v_size_norm\n",
    "    df_scores1 = lex_div + longword_voc_norm + v_size_norm\n",
    "    \n",
    "    ### Normalizing using formula \n",
    "    for df_score in df_scores:\n",
    "        df_scores_norm = np.append(df_scores_norm,(df_score - df_scores.min()) /\n",
    "                                                    (df_scores.max() - df_scores.min()))\n",
    "    return(df_scores,df_scores1,df_scores_norm)\n",
    "#     return(lex_div, longword_voc , voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dramatic Reader for Lower Grades by Florence Holbrook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "f=open('/home/ikenna/Documents/datascience/DS7337_NLP/27764-8.txt','r',encoding=\"ISO-8859-1\")\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Graded Poetry: Seventh Year by Georgia Alexander and Katherine Devereux Blake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/ikenna/Documents/datascience/DS7337_NLP/9542-8.txt','r',encoding=\"ISO-8859-1\")\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text1 = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sanders' Union Fourth Reader by Charles W. Sanders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/ikenna/Documents/datascience/DS7337_NLP/9542-8.txt','r',encoding=\"ISO-8859-1\")\n",
    "raw=f.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text2 = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = text_difficulty_score(text,text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Difficulty Score with long-word vocabulary size normalized\n",
      "- 0.12242019074488639\n",
      "- 2.211183114238104\n",
      "- 2.211183114238104\n"
     ]
    }
   ],
   "source": [
    "print(\"New Difficulty Score with long-word vocabulary size normalized\", *score[1],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Difficulty Score without long-word vocabulary size normailized\n",
      "- 161.1224201907449\n",
      "- 257.2111831142381\n",
      "- 257.2111831142381\n"
     ]
    }
   ],
   "source": [
    "print(\"New Difficulty Score without long-word vocabulary size normailized\", *score[0],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Normalized Difficulty Score\n",
      "- 0.0\n",
      "- 1.0\n",
      "- 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"New Normalized Difficulty Score\", *score[2],sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary size of all 3 text are 3992, 5703 and 17105; the lexical diversity is 0.1224, 0.2111 and 0.1186 for Text 1, Text 2 and Text 3 respectively. \n",
    "\n",
    "The difficulty scores are 161, 257, and 257 for Text 1, Text 2 and Text 3 respectively. Comparing this score to both lexical and vocabulary size, they are have diverent score rankings. I initially thought that any other scores could be closely related to lexical diversity score. The findings shows how long-word vocabulary size affects the difficulty score. The length of the word used for the long-word vocabulary size can be critical on the difficullty of the text.\n",
    "\n",
    "Overall the long-word vocabulary size is important and a good way to measure text difficulty. Lexical diversity is better because it takes into account the length of the text. For vocabulary size, if a text is large then the vocabulary size is most likely going to be large which would not be good. I think combining the 3 is better given that the vocabulary size would be normalized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
